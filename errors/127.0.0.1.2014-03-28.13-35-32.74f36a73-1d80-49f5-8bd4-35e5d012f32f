(dp1
S'output'
p2
S"<type 'exceptions.UnboundLocalError'> local variable 'tables' referenced before assignment"
p3
sS'layer'
p4
S'/home/hduser/web2py/applications/Hadoop/controllers/default.py'
p5
sS'code'
p6
S'# coding: utf8\n# try something like\nimport os\nimport subprocess\nimport MySQLdb\nTable = "workflow";\nmachine="localhost";\ndatabase_name="hadoop";\ndb_username="root";\npassword="hadoop";\n@auth.requires_login()\ndef index():\n#    return dict(a=1);\n    redirect(URL(\'all_jobs\'));\n\n    return dict(message="hello from default.py")\ndef compile1():\n    return dict(hello="hello");\ntkns=[]\ncodegen1="";\ndef codegen(str="",table=""):\n    posvars=request.post_vars;\n#    //codegen:          generate import-export code without performing actual import/export, modify the gnerated java file, compile it to jar, give it to sqoop.\n#    global $TABLE,$machine,$password,$db_username,$database_name;\n    global codegen1;\n    if str=="":\n       # os.remove(Table+".java");\n       # os.remove(Table+".jar");\n       # os.remove(Table+".class")\n\n        codegen1= "/home/hduser/sqoop/bin/sqoop codegen --connect jdbc:mysql://"+machine+"/"+database_name+" --username "+db_username+" --password \'"+password+"\' --table "+Table ;\n        if posvars[\'op\']=="import":\n            if posvars[\'field_delim\']!="":\n                codegen1=codegen1+" --fields-terminated-by "+"\'" +posvars[\'field_delim\']+"\'";\n            if posvars[\'line_delim\']!="":\n                codegen1=codegen1+" --lines-terminated-by "+"\'" +posvars[\'line_delim\']+"\'";\n            if \'enclosed\' in posvars:\n                if posvars[\'enclosingchar\']==\'"\':\n                    posvars[\'enclosingchar\']=\'\\"\';\n                    codegen1=codegen1+" "+posvars[\'enclosed\']+" \'"+posvars[\'enclosingchar\']+ "\'";\n\n    else:\n        os.remove(TABLE+".java");\n        os.remove(TABLE+".jar");\n        os.remove(TABLE+".class");\n        codegen1=str;\n\n    codegen1 = codegen1+" --bindir . 2> err ; echo $?";\n\n#    echo "codegen command=".$codegen."<br/>";\n    output = os.popen(codegen1).read()\n#    $output= shell_exec($codegen);\n    tkns=output.split(\'\\n\');\n\n #   if output[output.__len__()-2]!="0":\n    return dict (tkns=tkns,message = "codegen wright")\n#       return "<br>??????????????Codegen error<br>";#echo $output;exit(0);\n\n@auth.requires_login()\ndef all_jobs():\n    jobs=[];\n    output = os.popen(""" jps | grep Jps | awk \'{print $1;}\' """).read()\n    output = output.split(\'\\n\')[:-1];\n    try :\n        connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=\'jobs\')\n        cur=connection.cursor();\n        query = "select * from db where result = 0 AND job_status = 1";\n        cur.execute(query)\n        tables = cur.fetchall()\n        cur1 = connection.cursor();\n        query = "select * from db where job_status = 0";\n        cur1.execute(query)\n        tables2 = cur1.fetchall()\n        cur2 = connection.cursor();\n        query = "select * from db where result != 0";\n        cur2.execute(query)\n        tables3 = cur2.fetchall()\n#        max_id = tables[0][0];\n    except:\n        print "weee2"\n    return dict(L = tables,running = tables2,failed=tables3,db_type="sql");\n@auth.requires_login()\ndef sql1():\n    tables = [];\n    path = \'/\'\n    if \'path\' in request.get_vars:\n        path = request.get_vars[\'path\'];\n    a = "hadoop fs -ls "+path+" | tr -s \' \' | cut -d \' \' -f8"\n    output = os.popen(a).read()\n    tokens = output.split(\'\\n\')\n    error = ""\n    try :\n        connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=database_name)\n        cur=connection.cursor()\n        query = "show tables"\n        cur.execute(query)\n        tables = cur.fetchall()\n        dic ={}\n        for (t,) in tables:\n                    \n                    cur = connection.cursor()\n                    \n                    query = "desc "+t +";";\n                    cur.execute(query);\n                    \n                    columns = cur.fetchall()\n                    dic[t] = []\n                    for i in columns:\n                            dic[t].append(i[0])\n#                    print dic[t]\n        return dict(tokens=tokens,error=error,tables=tables,test = request.get_vars,dic=dic)\n    except:\n        print "weee"\n        error = "failed to connect to MySQL: "\n        return dict(tokens=tokens,error=error,tables=tables,dic=dic)\n@auth.requires_login()\ndef hdfs1():\n    tables = [];\n    path = \'/\'\n    if \'path\' in request.get_vars:\n        path = request.get_vars[\'path\'];\n    a = "hadoop fs -ls "+path+" | tr -s \' \' | cut -d \' \' -f8"\n    output = os.popen(a).read()\n    tokens = output.split(\'\\n\')\n    error = ""\n    try :\n        connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=database_name)\n        cur=connection.cursor()\n        query = "show tables"\n        cur.execute(query)\n        tables = cur.fetchall()\n        return dict(tokens=tokens,error=error,tables=tables,test = request.get_vars)\n    except:\n        print "weee"\n        error = "failed to connect to MySQL: "\n        return dict(tokens=tokens,error=error,tables=tables)\n@auth.requires_login()\ndef main():\n    if request.get_vars[\'dbtype\'] == "sql":\n        redirect(URL(\'sql1\'));\n    elif request.get_vars[\'dbtype\'] == "hdfs":\n        redirect(URL(\'hdfs1\'));\n@auth.requires_login()\ndef hdfs():\n    #this is export\n    present_pid = os.getpid();\n    global codegen1;\n    posvars = request.post_vars;\n    posvars[\'op\'] = "export";\n    comm = "sqoop "+posvars[\'op\'];\n    print posvars[\'updateid\'];\n\n    max_id = 0;\n    try :\n        connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=\'jobs\')\n        cur=connection.cursor();\n        query = "insert into db(operation,table_transfered,start_time,job_status) values (\'" + posvars[\'op\'] + "\'," + "\'" + posvars[\'table\'] +"\'," + "NOW(),0);";\n        print query;\n        cur.execute(query)\n        connection.commit();\n        cur1=connection.cursor();\n        query = """ select max(id) from db;  """;\n        cur1.execute(query)\n        tables = cur1.fetchall()\n        max_id = tables[0][0];\n    except:\n        print "weee2"\n        \n    if posvars[\'op\'] == "export":\n        comm=comm + " --connect jdbc:mysql://"+machine+"/"+database_name+" --username "+db_username+" --password \'"+password+"\' --table "+posvars[\'table\']+" "\n        \n        if \'directory\' in posvars:\n            comm=comm+" --export-dir "+posvars[\'directory\'];\n            \n        if posvars[\'updateid\']!="" and posvars[\'updateid\']!="Enter reference column to update the table":\n            comm=comm+" --update-key "+posvars[\'updateid\'] + " --update-mode allowinsert";\n\n#        comm=comm+" ; echo $?";\n        print "Export command="+comm+"<br>";\n                       ####this is the main execute step .... not done because sqoop not working properly\n\n        comm = comm+ """ ;mysql --user=root --password=hadoop -e "use jobs;update db set result=$?,job_status=1  where id= """+ str(max_id) +""" AND job_status=0 "; exit 1 """;\n        file1 = open(str(present_pid),\'w\');\n        print comm;\n        file1.write(comm);\n        file1.close();\n        pid = os.fork();\n        if pid != 0:\n                try :\n                        connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=\'jobs\')\n                        cur=connection.cursor();\n                        query = "update db set pid="+ str(pid+1) + " where id="+str(max_id)+";";\n                        cur.execute(query)\n                        connection.commit();\n                        tables = cur.fetchall()\n                except:\n                        print "weee1"\n\n        else:\n                os.execlp(\'bash\',\'bash\',str(present_pid));\n\n    val=\'0\';#output[output.__len__()-2];\n    redirect(URL(\'all_jobs\'))\n\n\n@auth.requires_login()\ndef sql():\n    print "came to sql"\n    #this is import\n    present_pid = os.getpid();\n    global codegen1;\n    posvars = request.post_vars;\n    posvars[\'op\'] = "import";\n    all_tables = posvars[\'table\'];\n    print posvars;\n    print type(all_tables);\n    if type(all_tables)==type("str"):\n        all_tables=[all_tables];\n    first_flag = 0;\n    for tab in all_tables:\n        comm = "sleep "+ str(first_flag) +" ;sqoop "+posvars[\'op\']+" ";\n        first_flag = first_flag+2;\n        posvars[\'table\'] = tab;\n        max_id = 0;\n        print "before"\n        print posvars[\'table\'];\n        try :\n            print "try came"\n            print posvars[\'table\'];\n            connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=\'jobs\')\n            cur=connection.cursor();\n            query = "insert into db(operation,table_transfered,start_time,job_status) values (\'" + posvars[\'op\'] + "\'," + "\'" + posvars[\'table\'] +"\'," + "NOW(),0);";\n            cur.execute(query)\n            connection.commit();\n            cur1=connection.cursor();\n            query = """ select max(id) from db;  """;\n            cur1.execute(query)\n            tables = cur1.fetchall()\n            max_id = tables[0][0];\n        except:\n            print "weee2"\n\n        if posvars[\'op\'] == "import":\n            imp="";\n            if \'encrypt\' in posvars:\n                codegen();\n                array=posvars[\'encrypt_columns\'].split(\',\');#explode(",",$_POST[\'encrypt_columns\']);\n                #print_r($array);\n    #            change_import($TABLE.".java",$array,$_POST[\'enckey\']);\n                compile1();\n                imp="  --jar-file "+Table+".jar --class-name "+Table;\n\n            comm=comm + " --connect jdbc:mysql://"+machine+"/"+database_name+" --username "+db_username+" --password \'"+password+"\' --append --table "+posvars[\'table\']+" "+imp;\n            if posvars[\'targetdir1\'] != "":\n                comm=comm+" --target-dir /"+posvars[\'targetdir1\'];\n            elif \'targetdir\' in posvars:\n                if posvars[\'targetdir\']==\'default\':\n                    comm=comm+" --target-dir /"+posvars[\'table\'];\n                elif posvars[\'targetdir\']!="":\n                    comm=comm+" --target-dir "+posvars[\'targetdir\'];\n            print query;\n            print "anath"\n#            print posvars[\'tab_columns\'];\n            if type(posvars[\'tab_columns\']) == type("apple"):\n                posvars[\'tab_columns\'] = [posvars[\'tab_columns\']];\n            columns = [];\n            yes = 0;\n            if \'tab_columns\' in posvars:\n                for i in posvars[\'tab_columns\']:\n                    a = i.split(\'$\');\n                    if a[0] == posvars[\'table\']:\n                        yes = 1;\n                        columns.append(a[1]);\n                if yes == 1:\n                    comm=comm+""" --columns " """ + columns[0];\n                for i in range(1,len(columns)):\n                    comm=comm+" ,"+columns[i];\n                comm=comm+""" " """;\n\n            if \'filetype\' in posvars:\n                comm=comm+ " "+posvars[\'filetype\'];\n\n            comm = comm+ """ ; mysql --user=root --password=hadoop -e "use jobs;update db set result=$?,job_status=1  where id= """+ str(max_id) +""" AND job_status=0 "; exit 1 """;\n            file1 = open(str(present_pid),\'w\');\n            file1.write(comm);\n            file1.close();\n            print comm;\n            print present_pid;\n            pid = os.fork();\n            if pid != 0:\n                    print pid;\n                    try :\n                            connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=\'jobs\')\n                            cur=connection.cursor();\n                            query = "update db set pid="+ str(pid+1) + " where id="+str(max_id)+";";\n                            cur.execute(query)\n                            connection.commit();\n                            tables = cur.fetchall()\n                    except:\n                            print "weee1"\n    \n            else:\n                    os.execlp(\'bash\',\'bash\',str(present_pid));\n\n    val=\'0\';#output[output.__len__()-2];\n    redirect(URL(\'all_jobs\'))\n#    return dict(posvars = request.post_vars);\ndef user():\n    return dict(form=auth())\n\nresponse._vars=response._caller(all_jobs)\n'
p7
sS'snapshot'
p8
(dp9
sS'traceback'
p10
S'Traceback (most recent call last):\n  File "/home/hduser/web2py/gluon/restricted.py", line 220, in restricted\n    exec ccode in environment\n  File "/home/hduser/web2py/applications/Hadoop/controllers/default.py", line 311, in <module>\n  File "/home/hduser/web2py/gluon/globals.py", line 385, in <lambda>\n    self._caller = lambda f: f()\n  File "/home/hduser/web2py/gluon/tools.py", line 3287, in f\n    return action(*a, **b)\n  File "/home/hduser/web2py/applications/Hadoop/controllers/default.py", line 81, in all_jobs\n    return dict(L = tables,running = tables2,failed=tables3,db_type="sql");\nUnboundLocalError: local variable \'tables\' referenced before assignment\n'
p11
s.