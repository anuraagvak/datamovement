(dp1
S'output'
p2
S"<type 'exceptions.TypeError'> cannot concatenate 'str' and 'Storage' objects"
p3
sS'layer'
p4
S'/home/hduser/web2py/applications/Hadoop/controllers/default.py'
p5
sS'code'
p6
S'# coding: utf8\n# try something like poooooooooooda\nimport os\nimport subprocess\nimport MySQLdb\nimport json\nimport urllib2\nTable = "workflow";\nmachine="localhost";\ndatabase_name="hadoop";\ndb_username="root";\npassword="hadoop";\npolicy_name = "anuraag1"\n@auth.requires_login()\ndef index():\n#    return dict(a=1);\n    redirect(URL(\'db_combination\'));\n#    redirect(URL(\'all_jobs\'));\n\n    return dict(message="hello from default.py")\ndef compile1():\n    return dict(hello="hello");\ntkns=[]\ncodegen1="";\ndef codegen():\n    print "the tables is"\n    print Table;\n    posvars=request.post_vars;\n#    //codegen:          generate import-export code without performing actual import/export, modify the gnerated java file, compile it to jar, give it to sqoop.\n#    global $TABLE,$machine,$password,$db_username,$database_name;\n    global codegen1;\n    try:\n    \tos.remove(Table+".java");\n    except:\n    \tprint ""\n    try:\n    \tos.remove(Table+".jar");\n    except:\n    \tprint ""\n    try:\n    \tos.remove(Table+".class")\n    except:\n    \tprint ""\n    codegen1= "sqoop codegen --connect jdbc:mysql://"+machine+"/"+database_name+" --username "+db_username+" --password \'"+password+"\' --table "+Table ;\n    """\n        if posvars[\'op\']=="import":\n            if posvars[\'field_delim\']!="":\n                codegen1=codegen1+" --fields-terminated-by "+"\'" +posvars[\'field_delim\']+"\'";\n            if posvars[\'line_delim\']!="":\n                codegen1=codegen1+" --lines-terminated-by "+"\'" +posvars[\'line_delim\']+"\'";\n            if \'enclosed\' in posvars:\n                if posvars[\'enclosingchar\']==\'"\':\n                    posvars[\'enclosingchar\']=\'\\"\';\n                    codegen1=codegen1+" "+posvars[\'enclosed\']+" \'"+posvars[\'enclosingchar\']+ "\'";\n    """\n    codegen1 = codegen1+"  2> err ; echo $?";\n\n    output = os.popen(codegen1).read()\n\n\ndef codechange(table,list_encrypt,list_mask,jsonobj):\n\tprint jsonobj;\n\tf = open(table+\'.java\');\n\tlines = f.readlines();\n\tf.close();\n\n\tf = open(\'encrypt_decrypt_functions.txt\');\n\tlines1 = f.readlines();\n\tf.close();\n\n\tf = open(\'mask.txt\');\n\tlines2 = f.readlines();\n\tf.close();\n\n\tf = open(\'unmask.txt\');\n\tlines3 = f.readlines();\n\tf.close();\n\n\tlines_mask = [];\n\tlines_mask1 = [];\n\n\n\n\tfor i in list_mask:\n\t\tlist_maps = jsonobj[\'hadoop\'][table][i][\'mask\'].split(\',\');\n\t\ttemp11 = "";\n\t\ttemp22 = "";\n\t\tfor j in list_maps:\n\t\t\ttemp11 = temp11 + \'mask_strings.put("\' + j.split(\':\')[0] + \'", "\' + j.split(\':\')[1] + \'");\'\n\t\t\ttemp22 = temp22 + \'mask_strings.put("\' + j.split(\':\')[1] + \'", "\' + j.split(\':\')[0] + \'");\'\n\t\tprint temp11\n\t\tprint temp22\n\t\t\t\n\t\ttemp_list = [\'public String mask_\'+i+\'(final String text){\\n\'] + [lines2[0]] + [temp11] +lines2[1:];\n\t\ttemp_list1 = [\'public String unmask_\'+i+\'(final String text){\\n\'] + [lines3[0]] + [temp22] +lines3[1:];\n\t\tlines_mask = lines_mask+temp_list;\n\t\tlines_mask1 = lines_mask1+temp_list1;\n\n\tconnection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=database_name)\n\tcur=connection.cursor()\n\tquery = "desc "+table;\n\tcur.execute(query)\n\ttables = cur.fetchall()\n\ta = [];\n\tfor i in tables:\n\t\ta.append(i[0]);\n\n\tlist_str=[];\n\tlist_int=[];\n\n\twrite_file=""\n\tflag1 = 1;\n\tstr1 = "";\n\tstr2 = \'\'.join(lines_mask) + \'\'.join(lines_mask1) + \'\'.join(lines1);\n\tfor l in lines:\n\t\tstr1 = l;\n\t\tflag = 0;\n\t\tif "SqoopRecord  implements DBWritable, Writable" in l:\n\t\t\tstr1="import java.util.HashMap; \\n"+str1+str2;\n\n\t\tif "get_" in l:\n\t\t\tif "String" in l:\n\t\t\t\tind1 = l.index("get_");\n\t\t\t\tind2 = l[ind1+4:].index("(");\n\t\t\t\tlist_str.append(l[ind1+4:ind1+4+ind2]);\n\t\t\t\n\t\t\tif "Integer" in l:\n\t\t\t\tind1 = l.index("get_");\n\t\t\t\tind2 = l[ind1+4:].index("(");\n\t\t\t\tlist_int.append(l[ind1+4:ind1+4+ind2]);\n\n\t\tif flag1 == 1:\n\t\t\tfor i in list_str:\n\t\t\t\tif "this."+i in l and i in list_encrypt:\n\t\t\t\t\tind1 = l.index("JdbcWritableBridge");\n\t\t\t\t\tind2 = l.index(";");\n\t\t\t\t\tstr1 = l[:ind1]+"encrypt1("+l[ind1:ind2]+");";\t#encrypt1 is for string\n\t\t\t\t\tflag = 1;\n\t\t\t\t\tbreak;\n\n\t\t\t\tif "this."+i in l and i in list_mask:\n\t\t\t\t\tind1 = l.index("JdbcWritableBridge");\n\t\t\t\t\tind2 = l.index(";");\n\t\t\t\t\tstr1 = l[:ind1]+"mask_"+i+"("+l[ind1:ind2]+");";\t#encrypt1 is for string\n\t\t\t\t\tflag = 1;\n\t\t\t\t\tbreak;\n\t\t\t\tif flag == 1:\n\t\t\t\t\tbreak;\n\t\t\tfor i in list_int:\n\t\t\t\tif "this."+i in l and i in list_encrypt:\n\t\t\t\t\tind1 = l.index("JdbcWritableBridge");\n\t\t\t\t\tind2 = l.index(";");\n\t\t\t\t\tstr1 = l[:ind1]+"encrypt2("+l[ind1:ind2]+");";\t#encrypt1 is for string\n\t\t\t\t\tflag = 1;\n\t\t\t\t\tbreak;\n\n\t\t\t\tif flag == 1:\n\t\t\t\t\tbreak;\n\n\t\tif flag1 == 1 and "}" in l:\n\t\t\tflag1 = 0;\n\n\t\tif "cur_result_set = __dbResults;" in l:\n\t\t\tflag1 = 1;\n\n\n\t\tif "JdbcWritableBridge.writeInteger" in l:\n\t\t\tind1 = l.index("JdbcWritableBridge.writeInteger");\n\t\t\ta = l[ind1:];\n\t\t\td = a.split(\',\');\n\t\t\tif d[0][32:] in list_encrypt:\n\t\t\t\tstr1 = a[:32]+\'decrypt2(\'+d[0][32:]+\')\'+a[len(d[0]):];\n\t\t\telse:\n\t\t\t\tstr1 = l;\n\n\t\tif "JdbcWritableBridge.writeString" in l:\n\t\t\tind1 = l.index("JdbcWritableBridge.writeString");\n\t\t\ta = l[ind1:];\n\t\t\td = a.split(\',\');\n\t\t\tif d[0][31:] in list_encrypt:\n\t\t\t\tstr1 = a[:31]+\'decrypt1(\'+d[0][31:]+\')\'+a[len(d[0]):];\n\t\t\telif d[0][31:] in list_mask:\n\t\t\t\tstr1 = a[:31]+\'unmask_\'+d[0][31:]+\'(\'+d[0][31:]+\')\'+a[len(d[0]):];\n\t\t\telse:\n\t\t\t\tstr1 = l;\n\n\t\twrite_file=write_file+str1;\n\t\tf = open(table+\'.java\',"w");\n\t\tf.write(write_file);\n\t\tf.close();\n\n\n@auth.requires_login()\ndef db_combination():\n\treturn dict(a=1);\n\t\n\n\n@auth.requires_login()\ndef all_jobs1():\n\tos.popen(\'python ~/web2py/ex.py\');\n\treturn dict(a=1);\n\n@auth.requires_login()\ndef all_jobs():\n    jobs=[];\n    output = os.popen(""" jps | grep Jps | awk \'{print $1;}\' """).read()\n    output = output.split(\'\\n\')[:-1];\n    try :\n        connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=\'jobs\')\n        cur=connection.cursor();\n        query = "select * from db where result = 0 AND job_status = 1";\n        cur.execute(query)\n        tables = cur.fetchall()\n        cur1 = connection.cursor();\n        query = "select * from db where job_status = 0";\n        cur1.execute(query)\n        tables2 = cur1.fetchall()\n        cur2 = connection.cursor();\n        query = "select * from db where result != 0";\n        cur2.execute(query)\n        tables3 = cur2.fetchall()\n#        max_id = tables[0][0];\n    except:\n        print "weee2"\n    return dict(L = tables,running = tables2,failed=tables3,db_type="sql");\n@auth.requires_login()\ndef sql1():\n    tables = [];\n    path = \'/\'\n    if \'path\' in request.get_vars:\n        path = request.get_vars[\'path\'];\n    a = "hadoop fs -ls "+path+" | tr -s \' \' | cut -d \' \' -f8"\n    output = os.popen(a).read()\n    tokens = output.split(\'\\n\')\n    error = ""\n    try :\n        connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=database_name)\n        cur=connection.cursor()\n        query = "show tables"\n        cur.execute(query)\n        tables = cur.fetchall()\n        dic ={}\n        for (t,) in tables:\n                    \n                    cur = connection.cursor()\n                    \n                    query = "desc "+t +";";\n                    cur.execute(query);\n                    \n                    columns = cur.fetchall()\n                    dic[t] = []\n                    for i in columns:\n                            dic[t].append(i[0])\n#                    print dic[t]\n        return dict(tokens=tokens,error=error,tables=tables,test = request.get_vars,dic=dic)\n    except:\n        print "weee"\n        error = "failed to connect to MySQL: "\n        return dict(tokens=tokens,error=error,tables=tables,dic=dic)\n@auth.requires_login()\ndef hdfs1():\n    tables = [];\n    path = \'/\'\n    if \'path\' in request.get_vars:\n        path = request.get_vars[\'path\'];\n    a = "hadoop fs -ls "+path+" | tr -s \' \' | cut -d \' \' -f8"\n    output = os.popen(a).read()\n    tokens = output.split(\'\\n\')\n    error = ""\n    try :\n        connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=database_name)\n        cur=connection.cursor()\n        query = "show tables"\n        cur.execute(query)\n        tables = cur.fetchall()\n        return dict(tokens=tokens,error=error,tables=tables,test = request.get_vars)\n    except:\n        print "weee"\n        error = "failed to connect to MySQL: "\n        return dict(tokens=tokens,error=error,tables=tables)\n@auth.requires_login()\ndef main():\n    if request.get_vars[\'dbtype\'] == "sql":\n        redirect(URL(\'sql1\'));\n    elif request.get_vars[\'dbtype\'] == "hdfs":\n        redirect(URL(\'hdfs1\'));\n\n@auth.requires_login()\ndef main1():\n    if request.get_vars[\'dbtype\'] == "sql":\n        redirect(URL(\'sql2\'));\n    elif request.get_vars[\'dbtype\'] == "cassandra":\n        redirect(URL(\'cassandra2\'));\n\n\ndef sql2():\n\treturn dict(a=100);\n\n\n@auth.requires_login()\ndef hdfs():\n    posvars = request.post_vars;\n    json_data=open(\'/home/hduser/web2py/applications/Policy_server/json_files/\'+posvars[\'policy_name\']+\'.json\',\'r\')\n    data = json.load(json_data)\n    print data[\'hadoop\']\n    json_data.close()\n    #this is export\n    present_pid = os.getpid();\n    global codegen1;\n    posvars[\'op\'] = "export";\n    comm = "sqoop "+posvars[\'op\'];\n    print posvars[\'updateid\'];\n\n    max_id = 0;\n    try :\n        connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=\'jobs\')\n        cur=connection.cursor();\n        query = "insert into db(operation,table_transfered,start_time,job_status) values (\'" + posvars[\'op\'] + "\'," + "\'" + posvars[\'table\'] +"\'," + "NOW(),0);";\n        print query;\n        cur.execute(query)\n        connection.commit();\n        cur1=connection.cursor();\n        query = """ select max(id) from db;  """;\n        cur1.execute(query)\n        tables = cur1.fetchall()\n        max_id = tables[0][0];\n    except:\n        print "weee2"\n        \n    imp = "";\n\n\n\n\n    if posvars[\'op\'] == "export":\n#        comm=comm + " --connect jdbc:mysql://"+machine+"/"+database_name+" --username "+db_username+" --password \'"+password+"\' --table "+posvars[\'table\']+" "\n    \ttables_encrypt = [];\n    \ttables_mask = [];\n\tif posvars[\'directory\'][1:] not in data[\'hadoop\']:\n\t\tdata[\'hadoop\'][posvars[\'directory\'][1:]] = {};\n\tprint posvars[\'directory\'][1:]\n\tprint "asdfasdfdkjasfhkasdhflkjadhfslkjhasdflkjdhasfklh"\n\tprint data\n\n    \tfor i in data[\'hadoop\'][posvars[\'directory\'][1:]]:\n\t\tprint i\n\t\tif data[\'hadoop\'][posvars[\'directory\'][1:]][i] == "encrypt":\n\t\t\ttables_encrypt.append(i);\n\t\t\tposvars[\'decrypt\'] = 1;\n\n\t\tif "mask" in data[\'hadoop\'][posvars[\'directory\'][1:]][i]:\n\t\t\ttables_mask.append(i);\n\t\t\tposvars[\'mask\'] = 1;\n\t\t\t\n        if \'decrypt\' in posvars or \'mask\' in posvars:\n\t        global Table;\n\t    \tTable = posvars[\'directory\'][1:];\n                codegen();\n\t\tcodechange(Table,tables_encrypt,tables_mask,data);\n\t\tcompile_code = """ /usr/lib/jvm/java-7-oracle/bin/javac -classpath /home/hduser/sqoop/sqoop-1.4.4.jar:/home/hduser/hadoop/hadoop-core-1.2.1.jar: """ + Table+".java; /usr/lib/jvm/java-7-oracle/bin/jar -cf "+Table+".jar "+Table+".class ";\n\t\toutput = os.popen(compile_code).read();\n                imp="  --jar-file "+Table+".jar --class-name "+Table;\n         \n\tcomm=comm + " --connect jdbc:mysql://"+machine+"/"+database_name+" --username "+db_username+" --password \'"+password+"\' --table "+posvars[\'table\']+" "+imp;\n\n\n        if \'directory\' in posvars:\n            comm=comm+" --export-dir "+posvars[\'directory\'];\n\n        if posvars[\'updateid\']!="" and posvars[\'updateid\']!="Enter reference column to update the table":\n            comm=comm+" --update-key "+posvars[\'updateid\'] + " --update-mode allowinsert";\n\n        print "Export command="+comm+" <br>";\n\n        comm = comm+ """ ;mysql --user=root --password=hadoop -e "use jobs;update db set result=$?,job_status=1  where id= """+ str(max_id) +""" AND job_status=0 "; exit 1 """;\n        file1 = open(str(present_pid),\'w\');\n        print comm;\n        file1.write(comm);\n        file1.close();\n        pid = os.fork();\n        if pid != 0:\n                try :\n                        connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=\'jobs\')\n                        cur=connection.cursor();\n                        query = "update db set pid="+ str(pid+1) + " where id="+str(max_id)+";";\n                        cur.execute(query)\n                        connection.commit();\n                        tables = cur.fetchall()\n                except:\n                        print "weee1"\n\n        else:\n                os.execlp(\'bash\',\'bash\',str(present_pid));\n\n    val=\'0\';#output[output.__len__()-2];\n    redirect(URL(\'all_jobs\'))\n\n\n@auth.requires_login()\ndef sql():\n    posvars = request.post_vars;\n    print "came to sql"+posvars\n    json_data=open(\'/home/hduser/web2py/applications/Policy_server/json_files/\'+posvars[\'policy_name\']+\'.json\',\'r\')\n    data = json.load(json_data)\n    print data[\'hadoop\']\n    json_data.close()\n    #this is import\n    present_pid = os.getpid();\n    global codegen1;\n    posvars[\'op\'] = "import";\n    all_tables = posvars[\'table\'];\n    print posvars;\n    print type(all_tables);\n    if type(all_tables)==type("str"):\n        all_tables=[all_tables];\n    first_flag = 0;\n    for tab in all_tables:\n        comm = "sleep "+ str(first_flag) +" ;sqoop "+posvars[\'op\']+" ";\n        first_flag = first_flag+2;\n        posvars[\'table\'] = tab;\n        max_id = 0;\n        print "before"\n        print posvars[\'table\'];\n        try :\n            print "try came"\n            print posvars[\'table\'];\n            connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=\'jobs\')\n            cur=connection.cursor();\n            query = "insert into db(operation,table_transfered,start_time,job_status) values (\'" + posvars[\'op\'] + "\'," + "\'" + posvars[\'table\'] +"\'," + "NOW(),0);";\n            cur.execute(query)\n            connection.commit();\n            cur1=connection.cursor();\n            query = """ select max(id) from db;  """;\n            cur1.execute(query)\n            tables = cur1.fetchall()\n            max_id = tables[0][0];\n        except:\n            print "weee2"\n\n        if posvars[\'op\'] == "import":\n            imp="";\n    \t    tables_encrypt = [];\n\t    tables_mask = [];\n\t    if posvars[\'table\'] not in data[\'hadoop\']:\n\t\t    data[\'hadoop\'][posvars[\'table\']] = {};\n       \t    for i in data[\'hadoop\'][posvars[\'table\']]:\n\t\tif data[\'hadoop\'][posvars[\'table\']][i] == "encrypt":\n\t\t\ttables_encrypt.append(i);\n\t\t\tposvars[\'encrypt\'] = 1;\t\n\t\tif "mask" in data[\'hadoop\'][posvars[\'table\']][i]:\n\t\t\ttables_mask.append(i);\n\t\t\tposvars[\'mask\'] = 1;\n\t\n\n\t    print tables_encrypt;\n\n    \t    if \'encrypt\' in posvars or \'mask\' in posvars:\n\t        global Table;\n\t    \tTable = posvars[\'table\'];\n                codegen();\n#                array=posvars[\'encrypt_columns\'].split(\',\');#explode(",",$_POST[\'encrypt_columns\']);\n                #print_r($array);\n                #change_import($TABLE.".java",$array,$_POST[\'enckey\']);\n#                compile1();\n\t\tcodechange(Table,tables_encrypt,tables_mask,data);\n\t\tcompile_code = """ /usr/lib/jvm/java-7-oracle/bin/javac -classpath /home/hduser/sqoop/sqoop-1.4.4.jar:/home/hduser/hadoop/hadoop-core-1.2.1.jar: """ + Table+".java; /usr/lib/jvm/java-7-oracle/bin/jar -cf "+Table+".jar "+Table+".class ";\n\t\toutput = os.popen(compile_code).read();\n\n                imp="  --jar-file "+Table+".jar --class-name "+Table;\n\n            comm=comm + " --connect jdbc:mysql://"+machine+"/"+database_name+" --username "+db_username+" --password \'"+password+"\' --append --table "+posvars[\'table\']+" "+imp;\n            if posvars[\'targetdir1\'] != "":\n                comm=comm+" --target-dir /"+posvars[\'targetdir1\'];\n            elif \'targetdir\' in posvars:\n                if posvars[\'targetdir\']==\'default\':\n                    comm=comm+" --target-dir /"+posvars[\'table\'];\n                elif posvars[\'targetdir\']!="":\n                    comm=comm+" --target-dir "+posvars[\'targetdir\'];\n            print query;\n            print "anath"\n#            print posvars[\'tab_columns\'];\n            if type(posvars[\'tab_columns\']) == type("apple"):\n                posvars[\'tab_columns\'] = [posvars[\'tab_columns\']];\n            columns = [];\n            yes = 0;\n            if \'tab_columns\' in posvars:\n                for i in posvars[\'tab_columns\']:\n                    a = i.split(\'$\');\n                    if a[0] == posvars[\'table\']:\n                        yes = 1;\n                        columns.append(a[1]);\n                if yes == 1:\n                    comm=comm+""" --columns " """ + columns[0];\n                for i in range(1,len(columns)):\n                    comm=comm+" ,"+columns[i];\n                comm=comm+""" " """;\n\n            if \'filetype\' in posvars:\n                comm=comm+ " "+posvars[\'filetype\'];\n\n            comm = comm+ """ ; mysql --user=root --password=hadoop -e "use jobs;update db set result=$?,job_status=1  where id= """+ str(max_id) +""" AND job_status=0 "; exit 1 """;\n            file1 = open(str(present_pid),\'w\');\n            file1.write(comm);\n            file1.close();\n            print comm;\n            print present_pid;\n            pid = os.fork();\n            if pid != 0:\n                    print pid;\n                    try :\n                            connection = MySQLdb.connect(host=\'localhost\',user=\'root\', passwd=\'hadoop\', db=\'jobs\')\n                            cur=connection.cursor();\n                            query = "update db set pid="+ str(pid+1) + " where id="+str(max_id)+";";\n                            cur.execute(query)\n                            connection.commit();\n                            tables = cur.fetchall()\n                    except:\n                            print "weee1"\n    \n            else:\n                    os.execlp(\'bash\',\'bash\',str(present_pid));\n\n    val=\'0\';#output[output.__len__()-2];\n    redirect(URL(\'all_jobs\'))\n#    return dict(posvars = request.post_vars);\ndef user():\n    return dict(form=auth())\n\nresponse._vars=response._caller(sql)\n'
p7
sS'snapshot'
p8
(dp9
sS'traceback'
p10
S'Traceback (most recent call last):\n  File "/home/hduser/web2py/gluon/restricted.py", line 220, in restricted\n    exec ccode in environment\n  File "/home/hduser/web2py/applications/Hadoop/controllers/default.py", line 530, in <module>\n  File "/home/hduser/web2py/gluon/globals.py", line 385, in <lambda>\n    self._caller = lambda f: f()\n  File "/home/hduser/web2py/gluon/tools.py", line 3287, in f\n    return action(*a, **b)\n  File "/home/hduser/web2py/applications/Hadoop/controllers/default.py", line 403, in sql\n    print "came to sql"+posvars\nTypeError: cannot concatenate \'str\' and \'Storage\' objects\n'
p11
s.